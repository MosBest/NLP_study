{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 导入模块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import jieba\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建语料库\n",
    "1. 读取数据\n",
    "2. 将data转为list\n",
    "3. 利用正则表达式(python re库) 去除无效字符（比如 '\\t' '\\n'），每个字符串变成一个个字符token\n",
    "4. 将每个划分后token重新组合成字符串\n",
    "5. 将list的所有字符串 组合成 一个字符串text。 将text称为 语料库"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 处理数据\n",
    "data = pd.read_csv(\"./datasource/export_sql_1558435.zip\", encoding='gb18030')\n",
    "content = list( data.content )\n",
    "content = [re.findall('[\\w|\\d]+', str(sentence)) for sentence in content]\n",
    "content = [' '.join(word) for word in content]\n",
    "\n",
    "text = ''\n",
    "for word in content:\n",
    "    text += word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建一个词的词频\n",
    "1. 切割预料库text,得到所有token\n",
    "2. 删除掉All_tokens 中 无效字符串 。' ' , 'n'\n",
    "3. 利用Counter统计每个词的词频 word_frequency\n",
    "    \n",
    "    word_frequency的内容是 （词， 词的总个数）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return jieba.lcut(string)\n",
    "\n",
    "All_tokens = cut(TEXT1)\n",
    "All_tokens = [i for i in All_tokens if i !=' ' and i!='n']\n",
    "word_frequency = Counter(All_tokens)\n",
    "#word_frequency.most_common(10)\n",
    "number_token = len(All_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 构建两个词组成的词组的词频\n",
    "1. 将词库中每相邻的两个词结合起来\n",
    "2. 利用Counter统计两个词组成的词组的词频 phrase_frequency 。用于计算 P(w1, w2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length_word = len(All_tokens)\n",
    "all_2gram_tokens = [ All_tokens[index] + All_tokens[index+1] for index in range(0, length_word-1)]\n",
    "phrase_frequency = Counter(all_2gram_tokens)\n",
    "# phrase_frequency是用于联合概率的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pro(word, word_frequency, all_sum):\n",
    "    \"\"\"\n",
    "    计算一个单词概率p(w1)\n",
    "    \"\"\"\n",
    "    if word in word_frequency:\n",
    "        return word_frequency[word] / all_sum\n",
    "    else:\n",
    "        return 1 / all_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def joint_prob(word, phrase_frequency, length_word):\n",
    "    \"\"\"\n",
    "    计算联合概率p(w1,w2)\n",
    "    \"\"\"\n",
    "    if word in phrase_frequency:\n",
    "        return phrase_frequency[word] / length_word\n",
    "    else:\n",
    "        return 1 / length_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def condition_prob(prior, post, word_frequency, phrase_frequency, length_word):\n",
    "    \"\"\"\n",
    "    计算条件概率p(w2|w1)\n",
    "    \"\"\"\n",
    "    return joint_prob(prior+post, phrase_frequency, length_word) / get_pro(prior, word_frequency, length_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_one_gram(string, word_frequency, all_sum):\n",
    "    \"\"\"\n",
    "    1-gram模型  P(sentence) = P(w1, w2, w3, w4, w5 ...) ~ P(w1) * P(w2) * P(w3) ...\n",
    "    \"\"\"\n",
    "    list_word = cut(string)\n",
    "    result = 1\n",
    "    for word in list_word:\n",
    "        result *= get_pro(word, word_frequency, all_sum)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.6841579371481411e-31"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_one_gram('大家好，enenen是hehehehehe', word_frequency, number_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Gram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def language_two_gram(string, word_frequency, phrase_frequency, length_word):\n",
    "    \"\"\"\n",
    "    2-gram模型。即 P (sentence) = P (w1, w2, w3, w4, w5 ...) ~ P(w1) * P(w2|w1) * P(w3|w2) ......\n",
    "    \"\"\"\n",
    "    string = cut(string)\n",
    "    result = get_pro(string[0], word_frequency, length_word)\n",
    "    \n",
    "    for index in range(0, len(string) - 1):\n",
    "        result *= condition_prob(string[index], string[index+1], word_frequency, phrase_frequency, length_word)\n",
    "    return result "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.542582815457654e-16"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_two_gram(\"真是一只好看的小猫\", word_frequency, phrase_frequency, length_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
