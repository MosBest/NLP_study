{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# how to get related words by word2vec?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Tree -> Similar Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = '/home/zhaodao/桌面/github/NLP_study/第二次内容/datasource/export_sql_1558435.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = pd.read_csv(csv_path, encoding='gb18030')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = content.fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = content['content'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut(string):\n",
    "    return ' '.join(jieba.cut(string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.409 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'这是 一个 测试'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cut('这是一个测试')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token(string):\n",
    "    return re.findall(r'[\\d|\\w]+', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['这是一个测试']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token('这是一个测试\\n\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = [token(n) for n in news_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = [' '.join(n) for n in news_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_content = [cut(n) for n in news_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "89611"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(news_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'骁龙 835 作为 唯一 通过 Windows   10 桌面 平台 认证 的 ARM 处理器   高通 强调   不会 因为 只 考虑 性能 而 去 屏蔽掉 小 核心   相反   他们 正 联手 微软   找到 一种 适合 桌面 平台 的   兼顾 性能 和 功耗 的 完美 方案   报道 称   微软 已经 拿到 了 一些 新 的 源码   以便 Windows   10 更好 地 理解 big   little 架构   资料 显示   骁龙 835 作为 一款 集成 了 CPU   GPU   基带   蓝牙   Wi   Fi 的 SoC   比 传统 的 Wintel 方案 可以 节省 至少 30   的 PCB 空间   按计划   今年 Q4   华硕   惠普   联想 将 首发 骁龙 835   Win10 电脑   预计 均 是 二合一 形态 的 产品   当然   高通 骁龙 只是 个 开始   未来 也许 还 能 见到 三星 Exynos   联发科   华为 麒麟   小米 澎湃 等 进入 Windows   10 桌面 平台'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_content[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news-sentences-cut.txt', 'w') as f:\n",
    "    for n in news_content:\n",
    "        f.write(n + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import LineSentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "news_word2ve = Word2Vec(LineSentence('news-sentences-cut.txt'), size=35, workers=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('意大利', 0.8953498601913452),\n",
       " ('捷克', 0.8746763467788696),\n",
       " ('克罗地亚', 0.8579184412956238),\n",
       " ('摩洛哥', 0.8542280197143555),\n",
       " ('瑞士', 0.8326007723808289),\n",
       " ('巴塞罗那', 0.8307761549949646),\n",
       " ('多哈', 0.8224089741706848),\n",
       " ('比利时', 0.8209236264228821),\n",
       " ('丹麦', 0.8152623772621155),\n",
       " ('西班牙', 0.8117986917495728)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2ve.most_similar('葡萄牙')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Date, Better Results\n",
    "\n",
    "1. 分词的问题\n",
    "2. **数据量** 数据越多，效果越好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('表示', 0.8999356031417847),\n",
       " ('指出', 0.8666229248046875),\n",
       " ('认为', 0.854762077331543),\n",
       " ('坦言', 0.8156627416610718),\n",
       " ('看来', 0.7983412146568298),\n",
       " ('介绍', 0.7638407945632935),\n",
       " ('明说', 0.7625210285186768),\n",
       " ('文说', 0.7567147016525269),\n",
       " ('告诉', 0.7466958165168762),\n",
       " ('透露', 0.7245856523513794)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2ve.most_similar('说')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('指出', 0.9132236838340759),\n",
       " ('说', 0.8999356031417847),\n",
       " ('认为', 0.8779436349868774),\n",
       " ('透露', 0.8141977190971375),\n",
       " ('坦言', 0.802665650844574),\n",
       " ('强调', 0.7953732013702393),\n",
       " ('介绍', 0.7485463619232178),\n",
       " ('看来', 0.7436285018920898),\n",
       " ('称', 0.7149076461791992),\n",
       " ('建议', 0.6876084804534912)]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2ve.most_similar('表示')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('认为', 0.7457494735717773),\n",
       " ('指出', 0.7184297442436218),\n",
       " ('要求', 0.7008938789367676),\n",
       " ('表示', 0.6876084804534912),\n",
       " ('强调', 0.6756607890129089),\n",
       " ('呼吁', 0.6513333916664124),\n",
       " ('解释', 0.6433683633804321),\n",
       " ('意见建议', 0.6357353329658508),\n",
       " ('评价', 0.632862389087677),\n",
       " ('鼓励', 0.6282050013542175)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_word2ve.most_similar('建议')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 根据已有的词汇，得到更多的词汇"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_related_words(initial_words, model):\n",
    "    \"\"\"\n",
    "    @initial_words are initial words we already know\n",
    "    @model is the word2vec model\n",
    "    \"\"\"\n",
    "\n",
    "    unseen = initial_words\n",
    "    \n",
    "    seen = defaultdict(int)\n",
    "    \n",
    "    max_size = 500\n",
    "    \n",
    "    while unseen and len(seen) < max_size:\n",
    "        if len(seen) % 50 == 0:\n",
    "            print('seen length : {}'.format(len(seen)))\n",
    "        node = unseen.pop(0)\n",
    "        \n",
    "        new_expanding = [w for w, s in model.most_similar(node, topn=20)]\n",
    "        \n",
    "        unseen += new_expanding\n",
    "        \n",
    "        seen[node] += 1 # 打分机制，可以优化\n",
    "        \n",
    "        ## 优化点：\n",
    "        #优化点1. score function could be revised\n",
    "        #优化点2. using dymanic programming to reduce computing time\n",
    "    return seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 0\n",
      "seen length : 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/ipykernel_launcher.py:18: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 100\n",
      "seen length : 150\n",
      "seen length : 150\n",
      "seen length : 200\n",
      "seen length : 200\n",
      "seen length : 250\n",
      "seen length : 300\n",
      "seen length : 350\n",
      "seen length : 350\n",
      "seen length : 400\n",
      "seen length : 400\n",
      "seen length : 450\n"
     ]
    }
   ],
   "source": [
    "related_words = get_related_words(['说', '表示'], news_word2ve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('表示', 89),\n",
       " ('指出', 88),\n",
       " ('说', 87),\n",
       " ('认为', 79),\n",
       " ('坦言', 69),\n",
       " ('透露', 64),\n",
       " ('所说', 52),\n",
       " ('看来', 50),\n",
       " ('明说', 50),\n",
       " ('告诉', 50),\n",
       " ('介绍', 47),\n",
       " ('文说', 43),\n",
       " ('强调', 41),\n",
       " ('提到', 41),\n",
       " ('称', 40),\n",
       " ('中说', 34),\n",
       " ('建议', 34),\n",
       " ('呼吁', 28),\n",
       " ('普遍认为', 28),\n",
       " ('写道', 26),\n",
       " ('提及', 25),\n",
       " ('申明', 23),\n",
       " ('特别强调', 23),\n",
       " ('中称', 21),\n",
       " ('觉得', 19),\n",
       " ('时说', 17),\n",
       " ('说道', 17),\n",
       " ('声称', 17),\n",
       " ('而言', 16),\n",
       " ('地说', 16),\n",
       " ('问', 16),\n",
       " ('深有体会', 16),\n",
       " ('来说', 15),\n",
       " ('相信', 13),\n",
       " ('事实上', 13),\n",
       " ('直言', 13),\n",
       " ('如是说', 12),\n",
       " ('看法', 11),\n",
       " ('表明', 11),\n",
       " ('但是', 11),\n",
       " ('重申', 11),\n",
       " ('敦促', 11),\n",
       " ('承认', 11),\n",
       " ('希望', 11),\n",
       " ('还是', 10),\n",
       " ('还称', 10),\n",
       " ('说法', 10),\n",
       " ('引用', 10),\n",
       " ('知道', 10),\n",
       " ('回答', 10),\n",
       " ('原话', 10),\n",
       " ('描述', 10),\n",
       " ('解释', 10),\n",
       " ('坦承', 9),\n",
       " ('评述', 9),\n",
       " ('对此', 9),\n",
       " ('批评', 9),\n",
       " ('谈到', 8),\n",
       " ('当然', 8),\n",
       " ('道', 7),\n",
       " ('管理司', 7),\n",
       " ('知情', 7),\n",
       " ('推测', 7),\n",
       " ('反复强调', 7),\n",
       " ('要说', 7),\n",
       " ('上称', 7),\n",
       " ('时称', 7),\n",
       " ('其实', 7),\n",
       " ('叙林', 6),\n",
       " ('条法司', 6),\n",
       " ('了然于心', 6),\n",
       " ('眼中', 6),\n",
       " ('资深', 6),\n",
       " ('了解', 6),\n",
       " ('获悉', 6),\n",
       " ('中山大学', 6),\n",
       " ('旅游局', 6),\n",
       " ('供图', 6),\n",
       " ('上海市', 6),\n",
       " ('诚揽', 6),\n",
       " ('管理局', 6),\n",
       " ('肿瘤医院', 6),\n",
       " ('武说', 6),\n",
       " ('洪说', 6),\n",
       " ('深有感触', 6),\n",
       " ('西万', 6),\n",
       " ('国利', 6),\n",
       " ('红霞', 6),\n",
       " ('八面玲珑', 6),\n",
       " ('祁', 6),\n",
       " ('巍巍', 6),\n",
       " ('保国', 6),\n",
       " ('农艺师', 6),\n",
       " ('永康', 6),\n",
       " ('瀛', 6),\n",
       " ('看到', 6),\n",
       " ('留意到', 6),\n",
       " ('南方日报', 6),\n",
       " ('北青报', 6),\n",
       " ('并不知道', 6),\n",
       " ('见到', 6),\n",
       " ('鲁金博', 6),\n",
       " ('张隽玮', 6),\n",
       " ('写信给', 6),\n",
       " ('童岚', 6),\n",
       " ('证实', 6),\n",
       " ('具名', 6),\n",
       " ('获知', 6),\n",
       " ('估计', 6),\n",
       " ('问道', 6),\n",
       " ('怼', 6),\n",
       " ('预言', 6),\n",
       " ('指责', 6),\n",
       " ('出马', 6),\n",
       " ('暗示', 6),\n",
       " ('说明', 6),\n",
       " ('抨击', 6),\n",
       " ('国务院发展研究中心', 5),\n",
       " ('韩长', 5),\n",
       " ('匿名', 5),\n",
       " ('援引', 5),\n",
       " ('的话', 5),\n",
       " ('宣称', 5),\n",
       " ('越通社', 5),\n",
       " ('一贯', 5),\n",
       " ('坚定性', 5),\n",
       " ('提出', 5),\n",
       " ('阐述', 5),\n",
       " ('更应', 5),\n",
       " ('把握', 5),\n",
       " ('遵循', 5),\n",
       " ('一说', 5),\n",
       " ('所指', 5),\n",
       " ('正如', 5),\n",
       " ('反问', 5),\n",
       " ('说出', 5),\n",
       " ('简短', 5),\n",
       " ('给出', 5),\n",
       " ('列举', 5),\n",
       " ('不在意', 5),\n",
       " ('要求', 5),\n",
       " ('意见建议', 5),\n",
       " ('评价', 5),\n",
       " ('鼓励', 5),\n",
       " ('必要', 5),\n",
       " ('提醒', 5),\n",
       " ('慎重', 5),\n",
       " ('规制', 5),\n",
       " ('为此', 5),\n",
       " ('适当', 5),\n",
       " ('质疑', 5),\n",
       " ('担忧', 5),\n",
       " ('谈论', 5),\n",
       " ('不过', 5),\n",
       " ('来讲', 5),\n",
       " ('无论', 5),\n",
       " ('所以', 5),\n",
       " ('张晓明', 4),\n",
       " ('客座', 4),\n",
       " ('国际关系学院', 4),\n",
       " ('北京大学医学部', 4),\n",
       " ('张志军', 4),\n",
       " ('全国人大常委会法制工作委员会', 4),\n",
       " ('谢伏瞻', 4),\n",
       " ('中共中央台办', 4),\n",
       " ('福兴', 4),\n",
       " ('王俪桦', 4),\n",
       " ('海峰', 4),\n",
       " ('清楚', 4),\n",
       " ('言', 4),\n",
       " ('部长级', 4),\n",
       " ('主张', 4),\n",
       " ('反对', 4),\n",
       " ('公敌', 4),\n",
       " ('施压', 4),\n",
       " ('谴责', 4),\n",
       " ('抵制', 4),\n",
       " ('向', 4),\n",
       " ('必要措施', 4),\n",
       " ('捕鲸', 4),\n",
       " ('令', 4),\n",
       " ('关注', 4),\n",
       " ('热议', 4),\n",
       " ('猜测', 4),\n",
       " ('归咎于', 4),\n",
       " ('促使', 4),\n",
       " ('指望', 4),\n",
       " ('坚信', 4),\n",
       " ('并非', 4),\n",
       " ('现阶段', 4),\n",
       " ('本身', 4),\n",
       " ('来看', 4),\n",
       " ('只是', 4),\n",
       " ('感兴趣', 4),\n",
       " ('看重', 4),\n",
       " ('毕竟', 4),\n",
       " ('感慨', 4),\n",
       " ('不久前', 3),\n",
       " ('库泰萨', 3),\n",
       " ('调频', 3),\n",
       " ('联访', 3),\n",
       " ('大使', 3),\n",
       " ('就此', 3),\n",
       " ('国台办', 3),\n",
       " ('对瓦姆', 3),\n",
       " ('中国外交部', 3),\n",
       " ('跨洋', 3),\n",
       " ('要防', 3),\n",
       " ('记者团', 3),\n",
       " ('称赞', 3),\n",
       " ('夸奖', 3),\n",
       " ('勋爵', 3),\n",
       " ('置评', 3),\n",
       " ('反驳', 3),\n",
       " ('期待', 3),\n",
       " ('认同', 3),\n",
       " ('理解', 3),\n",
       " ('实际上', 3),\n",
       " ('深感', 3),\n",
       " ('问过', 3),\n",
       " ('庆幸', 3),\n",
       " ('毫不', 3),\n",
       " ('远远不够', 3),\n",
       " ('非常重视', 3),\n",
       " ('口中', 3),\n",
       " ('感叹', 3),\n",
       " ('形容', 2),\n",
       " ('尔西', 2),\n",
       " ('希瑟', 2),\n",
       " ('贝特', 2),\n",
       " ('埃斯', 2),\n",
       " ('阿利', 2),\n",
       " ('里克', 2),\n",
       " ('奥布斯', 2),\n",
       " ('穆', 2),\n",
       " ('托尔', 2),\n",
       " ('顿说', 2),\n",
       " ('契奇', 2),\n",
       " ('布罗', 2),\n",
       " ('巴尔', 2),\n",
       " ('诺尔', 2),\n",
       " ('拉加', 2),\n",
       " ('普鲁', 2),\n",
       " ('雷泽盖', 2),\n",
       " ('佩林', 2),\n",
       " ('商务部长', 2),\n",
       " ('并称', 2),\n",
       " ('坚称', 2),\n",
       " ('伊原', 2),\n",
       " ('回击', 2),\n",
       " ('访俄', 2),\n",
       " ('出面', 2),\n",
       " ('喊话', 2),\n",
       " ('期望', 2),\n",
       " ('确信', 2),\n",
       " ('肯定', 2),\n",
       " ('让', 2),\n",
       " ('应该', 2),\n",
       " ('跟上', 2),\n",
       " ('或许', 2),\n",
       " ('将来', 2),\n",
       " ('做法', 2),\n",
       " ('角度看', 2),\n",
       " ('衡量', 2),\n",
       " ('赢家', 2),\n",
       " ('但', 2),\n",
       " ('想必', 2),\n",
       " ('平淡无奇', 2),\n",
       " ('大度', 2),\n",
       " ('震惊', 2),\n",
       " ('自认', 2),\n",
       " ('痛批', 2),\n",
       " ('下课', 2),\n",
       " ('看准', 2),\n",
       " ('顶天立地', 2),\n",
       " ('乐见', 2),\n",
       " ('坚持原则', 2),\n",
       " ('东说', 2),\n",
       " ('提法', 2),\n",
       " ('功不可没', 2),\n",
       " ('十分重视', 2),\n",
       " ('所言', 2),\n",
       " ('谎话', 2),\n",
       " ('直言不讳', 2),\n",
       " ('坚说', 2),\n",
       " ('归根结底', 2),\n",
       " ('心里', 2),\n",
       " ('眼里', 2),\n",
       " ('老友', 2),\n",
       " ('流泪', 2),\n",
       " ('一篇', 1),\n",
       " ('萨维尔', 1),\n",
       " ('演说', 1),\n",
       " ('做客', 1),\n",
       " ('大发雷霆', 1),\n",
       " ('一峰', 1),\n",
       " ('转达', 1),\n",
       " ('索尔', 1),\n",
       " ('薰', 1),\n",
       " ('斯说', 1),\n",
       " ('加里', 1),\n",
       " ('迈克', 1),\n",
       " ('伊戈尔', 1),\n",
       " ('邓', 1),\n",
       " ('副总干事', 1),\n",
       " ('兰斯', 1),\n",
       " ('斯特伦', 1),\n",
       " ('转述', 1),\n",
       " ('史蒂夫', 1),\n",
       " ('陈说', 1),\n",
       " ('步', 1),\n",
       " ('加塞', 1),\n",
       " ('听见', 1),\n",
       " ('千里', 1),\n",
       " ('孙家', 1),\n",
       " ('这条', 1),\n",
       " ('盒饭', 1),\n",
       " ('人行道', 1),\n",
       " ('长安街', 1),\n",
       " ('滨', 1),\n",
       " ('红楼', 1),\n",
       " ('办原', 1),\n",
       " ('顾', 1),\n",
       " ('祸害', 1),\n",
       " ('地方官', 1),\n",
       " ('小门', 1),\n",
       " ('阿贝巴叶', 1),\n",
       " ('司长', 1),\n",
       " ('司', 1),\n",
       " ('副司长', 1),\n",
       " ('马绍祥', 1),\n",
       " ('李波', 1),\n",
       " ('科技司', 1),\n",
       " ('张立群', 1),\n",
       " ('赋', 1),\n",
       " ('颜江瑛', 1),\n",
       " ('综合司', 1),\n",
       " ('范一飞', 1),\n",
       " ('学部委员', 1),\n",
       " ('董希淼', 1),\n",
       " ('周小川', 1),\n",
       " ('国家税务总局', 1),\n",
       " ('政务司', 1),\n",
       " ('研究部', 1),\n",
       " ('澄清', 1),\n",
       " ('揭秘', 1),\n",
       " ('未获', 1),\n",
       " ('日媒', 1),\n",
       " ('官网', 1),\n",
       " ('致歉', 1),\n",
       " ('官方', 1),\n",
       " ('同一天', 1),\n",
       " ('维基', 1),\n",
       " ('奥赖利', 1),\n",
       " ('观点', 1),\n",
       " ('措辞', 1),\n",
       " ('赞同', 1),\n",
       " ('表态', 1),\n",
       " ('言论', 1),\n",
       " ('评论', 1),\n",
       " ('举动', 1),\n",
       " ('台湾当局', 1),\n",
       " ('态度', 1),\n",
       " ('批判', 1),\n",
       " ('立场', 1),\n",
       " ('表述', 1),\n",
       " ('反省', 1),\n",
       " ('疑虑', 1),\n",
       " ('疑问', 1),\n",
       " ('然而', 1),\n",
       " ('当下', 1),\n",
       " ('尽管', 1),\n",
       " ('所谓', 1),\n",
       " ('是因为', 1),\n",
       " ('逻辑', 1),\n",
       " ('诟病', 1),\n",
       " ('孤例', 1),\n",
       " ('为何', 1),\n",
       " ('观察', 1),\n",
       " ('判断', 1),\n",
       " ('反映', 1),\n",
       " ('释放', 1),\n",
       " ('意味着', 1),\n",
       " ('迄今为止', 1),\n",
       " ('分析', 1),\n",
       " ('结论', 1),\n",
       " ('指向', 1),\n",
       " ('可能性', 1),\n",
       " ('取决于', 1),\n",
       " ('本来', 1),\n",
       " ('可是', 1),\n",
       " ('想想', 1),\n",
       " ('从没', 1),\n",
       " ('没什么', 1),\n",
       " ('做些', 1),\n",
       " ('明白', 1),\n",
       " ('咱们', 1),\n",
       " ('感觉', 1),\n",
       " ('挺', 1),\n",
       " ('想不到', 1),\n",
       " ('真是', 1),\n",
       " ('是不是', 1),\n",
       " ('谈不上', 1),\n",
       " ('了不起', 1),\n",
       " ('谈起', 1),\n",
       " ('说起', 1),\n",
       " ('溢于言表', 1),\n",
       " ('总是', 1),\n",
       " ('感触', 1),\n",
       " ('那位', 1),\n",
       " ('开玩笑', 1),\n",
       " ('感激', 1),\n",
       " ('腼腆', 1),\n",
       " ('不已', 1),\n",
       " ('坦荡', 1),\n",
       " ('欣喜', 1),\n",
       " ('无比', 1),\n",
       " ('佩服', 1),\n",
       " ('交情', 1),\n",
       " ('问起', 1),\n",
       " ('称呼', 1),\n",
       " ('记得', 1),\n",
       " ('想过', 1),\n",
       " ('来事', 1),\n",
       " ('姐夫', 1),\n",
       " ('恩师', 1),\n",
       " ('说完', 1),\n",
       " ('见过面', 1),\n",
       " ('谈过', 1),\n",
       " ('没想', 1),\n",
       " ('聊到', 1),\n",
       " ('祖父', 1),\n",
       " ('泪流满面', 1),\n",
       " ('拜访', 1),\n",
       " ('劝', 1),\n",
       " ('讨薪', 1),\n",
       " ('专业课', 1),\n",
       " ('教过', 1),\n",
       " ('告诫', 1),\n",
       " ('在意', 1),\n",
       " ('行里', 1),\n",
       " ('抱怨', 1),\n",
       " ('觉得很有', 1),\n",
       " ('打听', 1),\n",
       " ('听不懂', 1),\n",
       " ('跑题', 1),\n",
       " ('催', 1),\n",
       " ('失联案', 1),\n",
       " ('忍不住', 1),\n",
       " ('回忆起', 1),\n",
       " ('想起', 1),\n",
       " ('一脸', 1),\n",
       " ('直截了当', 1),\n",
       " ('哽咽', 1),\n",
       " ('惹', 1),\n",
       " ('离去', 1),\n",
       " ('骂', 1),\n",
       " ('喊', 1),\n",
       " ('说实话', 1),\n",
       " ('请问', 1),\n",
       " ('谈谈', 1),\n",
       " ('共见', 1),\n",
       " ('吗', 1),\n",
       " ('提问', 1),\n",
       " ('呢', 1),\n",
       " ('注意', 1),\n",
       " ('谈', 1),\n",
       " ('听', 1),\n",
       " ('答', 1),\n",
       " ('谈及', 1),\n",
       " ('李义东', 1),\n",
       " ('宝贵机会', 1),\n",
       " ('充满信心', 1),\n",
       " ('王昆义', 1),\n",
       " ('非常高兴', 1),\n",
       " ('思考', 1),\n",
       " ('来之不易', 1),\n",
       " ('回顾', 1),\n",
       " ('体奥', 1),\n",
       " ('借此机会', 1),\n",
       " ('感谢', 1),\n",
       " ('荣幸', 1),\n",
       " ('极好', 1),\n",
       " ('常说', 1),\n",
       " ('钟爱', 1),\n",
       " ('心中', 1),\n",
       " ('献给', 1),\n",
       " ('出身', 1),\n",
       " ('深知', 1),\n",
       " ('聊起', 1),\n",
       " ('留给', 1),\n",
       " ('耳熟能详', 1),\n",
       " ('情结', 1),\n",
       " ('哈尔滨人', 1),\n",
       " ('听听', 1),\n",
       " ('备感', 1),\n",
       " ('追忆', 1),\n",
       " ('因为', 1),\n",
       " ('不管', 1),\n",
       " ('显然', 1)]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(related_words.items(), key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 弱监督　＋启发式搜索　可以解决很多问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "向量化效果是关键。\n",
    "\n",
    "向量化　就是　表征（reprezation）\n",
    "\n",
    "如何将词向量化，如何将人向量化（社交媒体中）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# keywords 关键字\n",
    "## which words are important?\n",
    "1. 文本中出现的频率高\n",
    "2. 在本文中出现的频率高，但是在其他的场合出现的频率就很少\n",
    "\n",
    "## TF-IDF\n",
    "TF: Term Frequency , 即在文章中出现的频率\n",
    "\n",
    "DF: 包含单词T的文章的个数。　值越大，表明Ｔ的重要性越低\n",
    "\n",
    "idf: idf(t) = log(N/(df(t)))\n",
    "\n",
    "tf-idf = tf * idf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF Key words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'此外   自 本周   6 月 12 日   起   除 小米 手机 6 等 15 款 机型 外   其余 机型 已 暂停 更新 发布   含 开发 版   体验版 内测   稳定版 暂不受 影响   以 确保 工程师 可以 集中 全部 精力 进行 系统优化 工作   有人 猜测 这 也 是 将 精力 主要 用到 MIUI   9 的 研发 之中   MIUI   8 去年 5 月 发布   距今已有 一年 有余   也 是 时候 更新换代 了   当然   关于 MIUI   9 的 确切 信息   我们 还是 等待 官方消息'"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "news_content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_frequency(word):\n",
    "    return sum(1 for n in news_content if word in n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3376"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document_frequency('关于')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def idf(word):\n",
    "    \"\"\"We get the inversed document frequency\"\"\"\n",
    "    return math.log10(len(news_content) / document_frequency(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf('的') < idf(\"小米\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tf(word, document):\n",
    "    \"\"\"\n",
    "    Gets the term frequency of a @word in a @document.\n",
    "    \"\"\"\n",
    "    words = document.split()\n",
    "    return sum(1 for w in words if w==word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf('银行', news_content[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4550169427748936"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idf('银行')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keywords_of_a_doucment(document):\n",
    "    words = set(document.split())\n",
    "    \n",
    "    tfidf = [\n",
    "        (w, tf(w, document) * idf(w)) for w in words\n",
    "    ]\n",
    "    \n",
    "    tfidf = sorted(tfidf, key=lambda x:x[1], reverse=True)\n",
    "    \n",
    "    return tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_keywords_of_a_doucment(news_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  如何函数运行的慢，可以在juypter 上面用　% prun　来看程序的哪个地方(代码)运行的慢"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    }
   ],
   "source": [
    "% prun get_keywords_of_a_doucment(news_content[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 词云　wordcloud\n",
    "展示得好看写\n",
    "\n",
    "github: https://github.com/amueller/word_cloud\n",
    "\n",
    "wordcloud支持中文字体\n",
    "github:\n",
    "https://github.com/Computing-Intelligence/datasource\n",
    "中的SourceHanSerifSC-Regular.otf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_new_keywords = get_keywords_of_a_doucment(news_content[101])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "machine_new_keywords_dict = {w:score for w, score in machine_new_keywords}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wc = wordcloud.WordCloud('.../SourceHanSerifSC-Regular.otf')\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.imshow(wc.generate_from_frequencies(machine_new_keywords_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Vector Space: 向量空间。（word2vec, tfidf等等都是将信息表征成一个向量）\n",
    "\n",
    "## 如何将信息表征成计算机可以使用的东西\n",
    "\n",
    "\n",
    "tf-idf 不仅仅可以对word，也可以把整个文章弄成一个向量，再使用tf-idf。\n",
    "\n",
    "tf-idf缺点：\n",
    "    1. 无法解决同义词问题。比如将文章的“生气”改为“愤怒”，那么两个文章的tf-idf会很不一样。\n",
    "    2. 无法解决带有感情色彩文章的问题。比如“否定的否定”，“否定否定再否定”。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word2vec方法是一个抽象的的概念，上一节课的仅仅是一种word2vec。今年(2019)也提出了很多的word2vec方法(比如BERT)。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The importance of Representation 表征的重要性\n",
    "\n",
    "表征：　\n",
    "    \n",
    "    如何将信息变为计算机的可以认识的信息。\n",
    "    \n",
    "    如何将文本变向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ML其实只有两件事：表征＋决策"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01:53:52  继续"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "某篇文章　Ｄ1: ABCD\n",
    "某篇文章　Ｄ2: BAEF\n",
    "\n",
    "则文章的tf-idf: [a,b,c,d,e,f](里面每一个值表示对应大写的tf-idf)\n",
    "\n",
    "文章D1的tf-idf: [a1,b1,c1,d1,e1,f1]\n",
    "\n",
    "文章D2的tf-idf: [a2,b2,c2,d2,e2,f2]\n",
    "\n",
    "则D1和D2的相似性为：二者向量的cos余弦　cos = (v1 * v2)/(|v1| * |v2|)\n",
    "\n",
    "cos越大，夹角越小，　表示向量越相近\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TFIDF Vectorized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_num = 1000\n",
    "sub_samples = new_content[:sample_num]\n",
    "X = vectorized.fit_transform(sub_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(X[0].toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "document_id_1, document_id_2, document_id_3 = random.randint(0, 1000), random.randint(0, 1000), random.randint(0, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_of_d_1 = X[document_id_1].toarray()[0]\n",
    "vector_of_d_2 = X[document_id_2].toarray()[0]    \n",
    "vector_of_d_3 = X[document_id_3].toarray()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cosine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(v1, v2):\n",
    "    return cosine(v1, v2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(vector_of_d_3, vector_of_d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distance(vector_of_d_3, vector_of_d_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(range(1000)), key= lambda i:distance(vector_of_d_3, X[i].toarray()[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# boolean search　布尔搜索（也叫倒排索引）\n",
    "\n",
    "优点：\n",
    "\n",
    "    １．to process large document collections quickly.（快）\n",
    "    2. to allow more flexible matching operations\n",
    "    3. to allow ranked retrieval (page rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "建立一张表\n",
    "出现为１，没出现过为０\n",
    "\n",
    "     文章１　　　文章２　　文章３　　文章４　　文章５\n",
    "\n",
    "word1  1          1      0       0        1\n",
    "\n",
    "word2  0          1      1       1        0\n",
    "\n",
    "word3\n",
    "\n",
    "word4\n",
    "\n",
    "word5\n",
    "\n",
    "则在搜索引擎中，输入(word1 word2) 那么应该找哪个文章？\n",
    "\n",
    "直接　与and 即可　11001　and 01110 = 01000，则应该找文章2即可"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word1 : 11001，在计算机中其实可以直接用一个int就可以表示出11001。　即我们直接写出word1:25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 使用boolean search和tf-idf建立一个搜索引擎"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Search Engine\n",
    "\n",
    "Input : Words　关键词\n",
    "\n",
    "Output: Documnet　搜索得到的文章"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "d1, d2, d3 = {1, 2, 3}, {4, 5, 6, 3, 2}, {1, 3, 4}\n",
    "from operator import and_\n",
    "reduce(and_, [d1,d2,d3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_2_id = vectorized.vocabulary_\n",
    "id_2_word = {d: w for w, d in word_2_id.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transposed_x = X.transpace().toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def search_engine(query):\n",
    "    \"\"\"\n",
    "    @query is the searched words, splited by space\n",
    "    @return is the related documents which ranked by tfidf similarity\n",
    "    \"\"\"\n",
    "    words = query.split()\n",
    "    \n",
    "    query_vec = vectorized.transform([' '.join(words)]).toarray()[0]\n",
    "    \n",
    "    candidates_ids = [word_2_id[w] for w in words]\n",
    "    \n",
    "    documnets_ids = [\n",
    "        set(np.where(transposed_x[_id])[0] for _id in candidateds_ids)\n",
    "    ]\n",
    "    \n",
    "    from operator import and_\n",
    "    merged_documents = reduce(and_, documnets_ids)\n",
    "    \n",
    "    # we could know the documents which contain these words\n",
    "    \n",
    "    sorted_documents_id = sorted(merged_documents, key=lambda i: distance(query_vec, X[i].toarray()[0]))\n",
    "    return sorted_documents_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%timeit\n",
    "\n",
    "candidates_ids = search_engine('知否　绿肥　红瘦')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'**知否**asadsasd替换fsfadsf掉dsad**绿肥**dsd它们dddadsads**红瘦**dsdsadsa了'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"知否asadsasd替换fsfadsf掉dsad绿肥dsd它们dddadsads红瘦dsdsadsa了\"\"\"\n",
    "pat = r'(知否|绿肥|红瘦)'\n",
    "re.compile(pat).sub(repl=\"**\\g<1>**\", string=text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_query_pat(query):\n",
    "    return re.compile('({})'.format('|'.join(query.split())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_keywords(pat, document):\n",
    "    return pat.sub(repl=\"**\\g<1>**\", string=document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "highlight_keywords(get_query_pat('知否　绿肥　红瘦'), content['content'][111])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_engine_with_pretty_print(query):\n",
    "    candidates_ids = search_engine(query)\n",
    "    \n",
    "    for i, _id in enumerate(candidates_ids):\n",
    "        print('## Search Result {}'.format(i+1))\n",
    "        c = content['content'][_id]\n",
    "        highlight_keywords(get_query_pat(query), c)\n",
    "        print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_engine_with_pretty_print('知否　绿肥　红瘦')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PagePank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "先假设　所有网站同样重要\n",
    "\n",
    "然后，引用它的网站越多，说明这个网站越重要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
