{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Similarity: Word2Vec 词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编辑距离： Edit Distance\n",
    "\n",
    "可以用来衡量单词相似性，句子的差异大小。\n",
    "\n",
    "但是 其问题是：\n",
    "    1. 成都的简称叫蓉城，上海简称叫申城，有的时候很多词的意思是一样的，但是语言文字很不一样，这种情况Edit Distance是检测不出来的。\n",
    "    2. 再比如 美丽，美好，美的，如果用Edit Distance时是1，但是其实 美好和美丽一定比 美的和美丽 更接近。也就是说Edit Distance没有连续变化的情况。\n",
    "   3. 比如美丽和漂亮 应该 比 美丽和美好 要接近的多。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Edit Distance的好处：\n",
    "\n",
    "可解释性强。因为Edit Distance可以很准确的告诉你两个string1,string2在那个句子，那个词 那里 进行了改变。\n",
    "这样，在论文判重 就很有用处"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ｗord Embedding\n",
    "\n",
    "用于衡量两个单词的相似性\n",
    "\n",
    "词汇，文字在计算机中如何存储？\n",
    "\n",
    "how to represent words?\n",
    "\n",
    "ASCII:\n",
    "    a, b, c:97,98,99\n",
    "    \n",
    "    'abc':979899\n",
    "    \n",
    "Unicode:\n",
    "\n",
    "etc:UTF-8\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "why do not represent words as numbers?\n",
    "\n",
    "竟然计算机存储words使用数字(eg:ASCII a,97)，那么我们在自然语言处理中也可以在表示词words时不用数字表示？\n",
    "\n",
    "numeric:　连续的\n",
    "\n",
    "categorical: 离散的\n",
    "\n",
    "一般表示word的数字是离散的,连个word之间是没有联系的。\n",
    "\n",
    "如果表示word的数字是categotical, 那么我们就可以进一步用　one-hot 表示。\n",
    "one-hot有可能非常消耗电脑的存储空间。一个句子的one-hot就是一个有非常多0的矩阵。后来人名想用PCA或者SVD来优化这个稀疏矩阵，PCA的计算量很大。而且当有新词加入时，就需要把所有的东西重新运行一遍，这样效率就很低）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "但是words就真的只能是是categorical?\n",
    "\n",
    "如果用categorical表示两个单词，　那么这两个单词就没有联系。\n",
    "\n",
    "但是现实生活中，美　和　丽　之间　的关联性，一定比　美　和　定　关联性强。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何解决这个问题？\n",
    "\n",
    "我们希望把一个单词表征成一个向量，这样就可以运算。\n",
    "\n",
    "把一个单词表征成一个向量要求：\n",
    "１．　省空间\n",
    "２．　可以自己不断的调整\n",
    "３．　应该具有语义相似性"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们知道一个单词的周围单词的话，那么我们就可以知道这个单词的意思。\n",
    "\n",
    "对于一个单词，我们希望用周围的单词表现它。\n",
    "\n",
    "what is Embedding?\n",
    "在数学中，假设有三个实体。在某个向量空间中有\n",
    "｜E1 - E2｜ < ｜E1-E2｜　 ，　当将他们映射到其他空间向量，他们依然保持　这种关系｜f(E1) - f(E2)｜ < ｜f(E1)-f(E2)｜。那么我们就叫做embedding。\n",
    "\n",
    "word embedding: 单词的向量化\n",
    "\n",
    "graph embedding: 判断两个group社群的相似程度。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ICLR会议用于研究机器学习中　各种问题向量的表征问题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "１．　现在手里有的是一些文本（比如wiki)\n",
    "2.   我们期望的是有一种方法，每一个单词都可以用向量表征，且当两个词的意思相近，那么他们的向量也应该相近。\n",
    "\n",
    "词向量的原理：\n",
    "输入：　\n",
    "    １．一个文本\n",
    "    ２．文本中所有单词的one-hot编码\n",
    "    \n",
    "输出：\n",
    "    每个单词的向量\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "文本中 10k 个词\n",
    "那么某个词为x 的one-hot 是　1*10k\n",
    "假令　每个词的词向量大小为100\n",
    "\n",
    "则对于每一个词x, 有\n",
    "x * w1 + b1 = y1\n",
    "y1* w2 + b2 = y2\n",
    "\n",
    "x为某个词的one-hot 维度1*10k\n",
    "w1为所有词的词向量　维度10k * 100 (共10k个词)\n",
    "y1 维度为　1 * 100 就是词ｘ的词向量\n",
    "\n",
    "w2维度为　100*10k\n",
    "则y2维度为1*10k，在对这1*10k向量进行softmax操作\n",
    "\n",
    "当我们手里有个标准的ｙ,再求y和y2的交叉熵,即loss\n",
    "(y的构建，它相当于label。构建方法是　假设在一句话中词出现的顺序是　qwewadxgsadgg,词x的左右出现的词是d,g那么维度为1*10k的y的值是[0,0,0,0,0,..., 1(d的位置),...,1(g的位置), 0,0,0,0,0,...]。　具体是左右一个，还是两个，三个自己来定)\n",
    "\n",
    "用交叉熵对Ｗ１，Ｗ２求偏导数，从而更新参数W1，Ｗ２"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y = [0,0,0,1,0,1]\n",
    "\n",
    "y_ = [0.32,0.31,0.01,0,0,0.2]\n",
    "\n",
    "loss : 用交叉熵　-1 * sum(y[i] * np.log(y_[i]))\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "如果　y = 1 , y_ = 0.1 此时loss会很大\n",
    "　　　ｙ = 1 , y_ = 0.9 此时loss会很小，\n",
    "    而且0.1变到0.9 loss按指数减少的"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...漂亮...\n",
    "...好看...\n",
    "输入one-hot(漂亮)，　one-hot(好看)\n",
    "输出y2(漂亮)，　y2(好看)\n",
    "由于漂亮和好看词性相近，那么一般在句子中，他们周围出现的词也相近，那么上面的推到中，ｙ的值也相近。上面推到的实质就是利用梯度下降法找出好的Ｗ１，Ｗ２从而使得每一个词的y_接近y。因此漂亮和好看的y_就相近。由于Ｗ2,b2相同，则漂亮和好看的y1就相近。又由于b1相同，那么x* w1也相似。\n",
    "x表示某个词的one-hot操作。\n",
    "词漂亮的one-hot是[0,0,0,0,0,1,0,0,...]\n",
    "词好看的one-hot是[1,0,0,0,0,0,0,0,...]\n",
    "那么x*W1,\n",
    "则表明W1的第１行和第６行的值是相似的。\n",
    "以后Ｗ1的第一行就是词　好看　的词向量\n",
    "Ｗ1的第六行就是词　漂亮　的词向量。\n",
    "\n",
    "即证明，词意思相近的漂亮和好看的词向量也相近。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个思维很重要\n",
    "以前的机器学习，我们都是期望模型的输出是什么！\n",
    "但是这里word2ver虽然也是设计loss，也是梯度下降迭代，但是我们的目标不得模型的输出y，而是想知道前面的表征的内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "词向量其他概念   　\n",
    "       \n",
    "       1.HIERACHY SOFTMAX \n",
    "       2.NEGTIVE SAMPLING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何获得词向量\n",
    "\n",
    "python库　gendim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "词向量＋搜索　从而找出所有与“说”相近的词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ＮER　：　命名实体识别\n",
    "区分出来什么是人名，　组织机构名，　地名"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
